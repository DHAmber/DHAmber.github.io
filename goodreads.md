---
layout: page
title: 
permalink: /goodreads/
---


<p align="center">
Interesting Graph and Machine Learning papers
</p>

### Large Scale Machine learning

"[GraphSAINT Graph Sampling Based Inductive Learning Method](www.openreview.net/pdf?id=BJe8pkHFwS)"
<br>           Zeng et al. -<b> ICLR 2020</b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. Samples sub-graphs and runs GCN on entire sub-graph without node sampling in layers.
<br>         b. Defined node and edge sampling procedure in order to avoid bias.
  
<br>




### Combinatorial Optimization

"[Learning Combinatorial Optimization Algorithms over Graphs](https://arxiv.org/abs/1704.01665)"
<br>           Dai et al. -<b> NeurIPS 2017</b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. GCN + RL End to End framework for Comb Optimization.
  <br>       b. Tackled problems like TSP, Vertex Cover, Set Cover etc.
<br>

"[Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search](https://arxiv.org/abs/1810.10659)"
<br>           Lee et al. -<b> NeurIPS 2018</b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. GCN + Guided Tree Search.
  <br>       b. Parallelizable 
<br>

"[Attention, Learn to Solve Routing Problems!](https://arxiv.org/abs/1810.10659)"
<br>           Kool et al. -<b> ICLR 2019</b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. GCN + Transformer to encode Graph
  <br>       b. Decoder to output sequence of nodes.
<br>





### GNN

"[Position-aware Graph Neural Networks](https://arxiv.org/pdf/1906.04817.pdf)"
<br>           You et al. -<b> ICML 2019</b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. Two nodes, even if they have same neighboorhood get different embeddings. This is not in the case of GraphSage.
<br>         b. Uses Landmark nodes and weighs message from node to landmark nodes using distance to landmark node.
  
<br>

 "[Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/pdf/1806.08804.pdf)"
<br>           Ying et al. <b></b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. Aggregates nodes into clusters.
<br>         b. Coarsens clusters into larger level clusters.
  

"[Graph Attention Networks](https://arxiv.org/pdf/1710.10903.pdf)"
<br>           Velickovic et al. -<b> ICLR 2018</b>.
 <br><br>
  <b>Contribution:</b>
  <br>       a. Aggregate neighbor information based upon importance.
<br>         b. Inductive approach/
  
  
  
"[Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216.pdf)"
<br>           Hamilton et al. -<b> NeurIPS 2017</b>.
 <br><br>
  <b>Contribution:</b>
  <br>      a. Inductive approach
<br>        b. Learns aggregation matrices.
<br>        c. Samples neighorhood uniformly.




### Recommendation Systems using Graphs
"[Graph Convolutional Neural Networks for Web-Scale Recommender Systems] (https://arxiv.org/pdf/1806.01973.pdf)"
<br>           Ying et al. -<b> KDD 2018</b>.
 <br><br>
  <b>Contribution:</b>
  <br>      a. Inductive approach
<br>         b. Learns aggregation matrices.
<br>        c. Samples neighorhood uniformly.


"[Session-based Social Recommendation via Dynamic Graph Attention Networks](https://arxiv.org/pdf/1902.09362.pdf)"
<br>           Song et al. -<b> WSDM 2019</b>.
 <br><br>
  <b>Contribution:</b>
  <br>  a. Dynamic user interests and context-dependent social influences.
<br>    b. Attention based modelling




  
